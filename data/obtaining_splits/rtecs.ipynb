{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook generates train / valid / test (0.8 / 0.1 / 0.1) splits for RTECS, based on the random splitting method in MoleculeNet\n",
    "\n",
    "Splits were created for seeds of 122, 123, 124 (same seeds as MoleculeNet), using:\n",
    "\n",
    "- MoleculeNet Splitting method: https://github.com/deepchem/deepchem/blob/master/deepchem/splits/splitters.py\n",
    "\n",
    "RTECS dataset is a commericial dataset provided by Biovia. We used the acute oral toxicity data in mice to create binary classes of \"toxic\"/\"nontoxic\" chemicals using the LD50 (lethal dose for 50% of population) data. The cutoff used was 5000 mg/kg, as defined by EPA.\n",
    "\n",
    "However, since this is a commericial dataset, we cannot provide it. Instead below is the method used to create the binary classes and splits. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import time\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "# Required RDKit modules\n",
    "import rdkit as rd\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# modeling\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed value \n",
    "seed_value = 122 #122 123 124, as used in MoleculeNet\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bits for morgan fingerprints\n",
    "morgan_bits = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of radius for morgan fingerprints\n",
    "morgan_radius = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_oral_file = # cannot provide\n",
    "\n",
    "a_oral_data = pd.read_csv(a_oral_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define binary labels based on LD50 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_non_toxic_catg(numeric_mgkg):\n",
    "    ''' Defines binary classes for acute oral toxicity data, based on the LD50 (lethal dose for 50% of population) values.\n",
    "        Cutoff of 5000 mg/kg is used to define \"toxic\" and \"nontoxic\" molecules. \n",
    "    '''\n",
    "    if(numeric_mgkg <= 5000):\n",
    "        # Toxic\n",
    "        return \"toxic_a_oral\"\n",
    "    elif(numeric_mgkg > 5000):\n",
    "        # Nontoxic\n",
    "        return \"non-toxic_a_oral\"\n",
    "    \n",
    "a_oral_data['toxic_catg_5000'] = a_oral_data[\"numeric_mgkg\"].apply(lambda x: binary_non_toxic_catg(x))\n",
    "\n",
    "# Extract only EPA_catg, smiles and seqnum from a_oral_data \n",
    "a_oral_data = a_oral_data[['toxic_catg_5000','pubchem_CASRN_SMILES', 'CASRN_canonical_SMILES', 'seqnum']]\n",
    "\n",
    "# Convert the EPA_catg to one-hot encoded columns for labels to classify into\n",
    "a_oral_data_toxic_labels = pd.get_dummies(a_oral_data.toxic_catg_5000)\n",
    "a_oral_data = pd.concat([a_oral_data, a_oral_data_toxic_labels], axis=1)\n",
    "a_oral_data = a_oral_data.drop(['toxic_catg_5000'], axis=1)\n",
    "\n",
    "# SMILES for the molecules in the dataset had been curated by matching CASRN to Pubchem\n",
    "a_oral_data = a_oral_data.rename(columns = {'pubchem_CASRN_SMILES':'smiles', \n",
    "                                            'CASRN_canonical_SMILES':'canonical_smiles'})\n",
    "a_oral_data = a_oral_data.drop(['canonical_smiles'], axis=1)\n",
    "a_oral_data_seqnum = a_oral_data\n",
    "a_oral_data = a_oral_data.drop(['seqnum'], axis=1)\n",
    "a_oral_data = a_oral_data.drop_duplicates()\n",
    "\n",
    "# Selecting only EPA_catg as labels, i.e., 1 - Toxic, 0 - NonToxic by 5000 mg/kg LD50 cutoff \n",
    "a_oral_tasks = ['toxic_a_oral'] \n",
    "\n",
    "print(\"Acute oral tasks: %s\" % str(a_oral_tasks))\n",
    "print(\"%d tasks in total\" % len(a_oral_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting all tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [a_oral_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = a_oral_tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SMILES to CANONICAL SMILES\n",
    "# In the process of canonicalizing SMILES, any bad SMILES definition \n",
    "#     is caught and removed from the dataset\n",
    "\n",
    "for i in range(len(data)):\n",
    "    smis = list(data[i].smiles)\n",
    "\n",
    "    cans = []\n",
    "    for smi in smis:\n",
    "        mol = rd.Chem.MolFromSmiles(smi)\n",
    "        # see whether can be parsed to mol\n",
    "        if mol:\n",
    "            can = rd.Chem.MolToSmiles(mol, True)\n",
    "            cans.append(can)\n",
    "        else:\n",
    "            cans.append(np.nan)\n",
    "\n",
    "    data[i]['SMILES'] = cans\n",
    "    \n",
    "    # drop data point that has invalid molecule\n",
    "    data[i] = data[i][data[i]['SMILES'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MoleculeNet Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method borrowed from MoleculeNet for random splits of 0.8 / 0.1 / 0.1, train / test / valid \n",
    "# Returns index of random train, test, valid datasets in array of [train,test,valid]\n",
    "\n",
    "def split(dataset,\n",
    "            seed=None,\n",
    "            frac_train=.8,\n",
    "            frac_valid=.1,\n",
    "            frac_test=.1,\n",
    "            log_every_n=None):\n",
    "    \"\"\"\n",
    "        Splits internal compounds randomly into train/validation/test.\n",
    "        \"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.)\n",
    "    if not seed is None:\n",
    "        np.random.seed(seed)\n",
    "    num_datapoints = len(dataset)\n",
    "    train_cutoff = int(frac_train * num_datapoints)\n",
    "    valid_cutoff = int((frac_train + frac_valid) * num_datapoints)\n",
    "    shuffled = np.random.permutation(range(num_datapoints))\n",
    "    return (shuffled[:train_cutoff], shuffled[train_cutoff:valid_cutoff],\n",
    "            shuffled[valid_cutoff:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_data = []\n",
    "temp_test_data = []\n",
    "temp_valid_data = []\n",
    "for i in range(len(data)):\n",
    "    splitter_i = split(data[i])\n",
    "    for j in range(len(splitter_i)):\n",
    "            if j==0: \n",
    "                temp_train_data.append(data[i].iloc[splitter_i[j]])\n",
    "            if j==1: \n",
    "                temp_test_data.append(data[i].iloc[splitter_i[j]])\n",
    "            if j==2: \n",
    "                temp_valid_data.append(data[i].iloc[splitter_i[j]])\n",
    "                \n",
    "train_data = temp_train_data[0]\n",
    "test_data  = temp_test_data[0]\n",
    "valid_data  = temp_valid_data[0]\n",
    "\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    train_data = train_data.merge(temp_train_data[i], how='outer', on='smiles')\n",
    "    test_data  = test_data.merge(temp_test_data[i], how='outer', on='smiles')\n",
    "    valid_data  = valid_data.merge(temp_valid_data[i], how='outer', on='smiles')\n",
    "\n",
    "\n",
    "data = [train_data, test_data, valid_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = #f\"pathway-to-data/rtecs/split_data/seed_{seed_value}/\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data[0], data_path+\"train_data_rtecs.pth\")\n",
    "torch.save(data[1], data_path+\"test_data_rtecs.pth\")\n",
    "torch.save(data[2], data_path+\"valid_data_rtecs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of examples, train: \" + str(data[0].shape[0]))\n",
    "print(\"Total number of examples, test: \" + str(data[1].shape[0]))\n",
    "print(\"Total number of examples, valid: \" + str(data[2].shape[0]))\n",
    "print(\"Total number of examples, train+test+valid: \" + str(data[0].shape[0] + data[1].shape[0] + data[2].shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check: load saved rtecs train/test/valid data\n",
    "train_data=torch.load(rtecs_data_path + 'train_data_rtecs.pth')\n",
    "test_data=torch.load(rtecs_data_path + 'test_data_rtecs.pth')\n",
    "valid_data=torch.load(rtecs_data_path + 'valid_data_rtecs.pth')\n",
    "\n",
    "data = [train_data, test_data, valid_data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
